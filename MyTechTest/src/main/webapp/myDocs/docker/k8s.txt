yinslgo@gmail.com/yinsl123

pod(rc):
spec.containers[].ports[]：容器需要暴露的端口号列表
spec.containers[].ports[].containerPort：容器需要监听的端口号
spec.containers[].ports[].hostPort：容器所在主机需要监听的端口号，默认与containerPort相同

service:
spec.ports[]：service需要暴露的端口号列表
spec.ports[].port：服务监听的端口号
spec.ports[].targetPort：需要转发到后端pod的端口号
spec.ports[].nodePort：节点暴露的端口号，提供对外访问能力，端口范围：30000~32767



k8s网络配置方案：
直接路由、flannel、Open vSwitch

centos7-private-repository 10.25.31.102 docker registry
centos7  10.25.31.44      master
centos72 10.25.31.45     slave
centos73 10.25.31.109    slave

centos74 10.25.31.33     单机实现权威指南



在10.25.31.33上安装k8s

安装kubectl：
下载：https://github.com/kubernetes/kubernetes/releases/download/v1.2.4/kubernetes.tar.gz
tar zxvf kubernetes.tar.gz
cp kubernetes/platforms/linux/amd64/kubectl /usr/local/bin/kubectl
chmod +x /usr/local/bin/kubectl

cp kubernetes/platforms/linux/amd64/kubectl /usr/bin/kubectl
chmod +x /usr/bin/kubectl

[root@localhost cluster]# export K8S_VERSION=$(curl -sS https://storage.googleapis.com/kubernetes-release/release/stable.txt)
[root@localhost cluster]# echo $K8S_VERSION
v1.2.4


运行：
export ARCH=amd64
docker run -d \                        #-d, --detach=true|false，后台模式，默认为false。后台模式的容器不能使用 --rm选项
    --volume=/:/rootfs:ro \
    --volume=/sys:/sys:ro \
    --volume=/var/lib/docker/:/var/lib/docker:rw \
    --volume=/var/lib/kubelet/:/var/lib/kubelet:rw \
    --volume=/var/run:/var/run:rw \
    --net=host \        #设置容器的网路模式：
                        # 'bridge': creates a new network stack for the container on the docker bridge
                        # 'none': no networking for this container
                        # 'container:<name|id>': reuses another container network stack
                        # 'host': use the host network stack inside the container.  
                        # Note: the host mode gives the container full access to local system services such as D-bus
                        # and is therefore considered insecure.
                                            
    --pid=host \        #设置容器的PID模式，host意味着在容器内不使用宿主机的PID命名空间，host模式给予容器完全的权限访问宿主机的PID，因此是不安全的
    --privileged \                                                   #特权，默认为false。一般不允许访问设备。给予特权的容器可以访问任何设备。
    gcr.io/google_containers/hyperkube-${ARCH}:${K8S_VERSION} \    #docker image
    /hyperkube kubelet \
        --containerized \
        --hostname-override=127.0.0.1 \
        --api-servers=http://localhost:8080 \
        --config=/etc/kubernetes/manifests \
        --cluster-dns=10.0.0.10 \                        #部署dns
        --cluster-domain=cluster.local \                 #部署dns
        --allow-privileged --v=2
        

k8s命令：
[root@localhost kubernetes]# kubectl config set-cluster test-doc --server=http://localhost:8080
cluster "test-doc" set.
[root@localhost kubernetes]# kubectl config set-context test-doc --cluster=test-doc
context "test-doc" set.
[root@localhost kubernetes]# kubectl config use-context test-doc
switched to context "test-doc".


查看节点列表：
kubectl get nodes
[root@k8s-master apps]# kubectl get nodes
NAME         STATUS    AGE
k8s-node01   Ready     4d
k8s-node02   Ready     48d


设计：
pods：k8s中的基本调度单元就是pod。一个pod由一个或多个容器（container）组成，保证在主机的同一个位置并能共享资源。每个pod都被分配了一个唯一（集群内）的IP地址。
保证了应用程序使用端口时没有冲突的风险。一个pod可以定义一个卷（volume），例如一个本地磁盘目录或网络磁盘，并且把它暴露给pod里的容器。pod可以通过k8s api手动管理，
或者把管理委托给控制器。

Labels and Selectors（标签与标签选择器）：k8s允许客户端（用户或内部组件）附加key-value对，称为标签，给系统中的任意api对象，例如pods和nodes。相应的，标签选择器
紧靠标签，解决匹配对象的查询。标签与选择器是k8s的主要分组机制，经常用于决定那些组件可以应用哪些操作。
例如：如果一个应用的pods有一个标签“tier”（front-end，back-end等等）和“release_track”（canary，production等等），那么在所有的back-end canary nodes上都有一个
操作可以使用选择器：tier=back-end AND release_track=canary

Controllers：控制器是一个调节回路，它将实际的集群状态导向期望的集群状态。它通过管理一组pods来实现这一点。有一种控制器是复制控制器，它通过运行特定数量的副本，在
集群中复制和缩放。当节点中的pods运行失败的时候，它也可以创建复制pods。其他控制器是k8s系统核心的一部分，包括一个daemonSet控制器，用于在一台机器（或一些机器的子集）
上只运行一个pod，还有一个job控制器，用于运行pods直至完成，例如，作为批量处理的一部分。控制器管理的pods是由标签选择器决定的，作为控制器定义的一部分。

Services：一个k8s的服务是一组相互协作的pods，例如一个多层应用的一层。构成一个服务的一组pods由标签选择器定义。k8s通过给服务分配一个固定的ip和dns名称来提供
服务发现和请求路由，以及round-robin的负载均衡连接到ip地址，通过选择器匹配到pods（即使失败导致pods从机器中移除）。默认情况下一个服务暴露在集群内部（例如，后端pods
可能划分为一个服务，来自前端pods的请求通过负载均衡到达它们），但是一个服务也可以暴露在集群外部（例如，客户端到达前端pods）。

架构：
k8s节点：是由k8s管理的单机（或虚拟机）。集群中的每个节点必须运行容器（例如docker），以及kubelet和kube-proxy。kubelet按控制面（control plane）的指示负责
启动、停止、管理应用容器（组织进pods）。kube-proxy是网络代理和负载均衡的一个实现，并支持与其他网络操作的服务抽象。

Kubernetes control plane：由几个部分组成，每个都有自己的进程，运行在一个单一的主节点。apiserver使用基于http的json为k8s api提供服务，它为k8s提供内部和外部的接口。
apiserver将集群状态写入etcd，持久化数据存回k8s。调度器是可插拔的组件，它选择一个节点上的一个临时的pod来运行。控制器管理器是核心k8s控制器运行在其中的进程。控制器
与apiserver通信来创建、更新和删除它们管理（pods，service endpoints等等）的资源。


k8s配置：
用配置文件发布一个容器：
hello-world.yaml
apiVersion: v1  #目前只支持以下版本："batch/v1" "autoscaling/v1" "authorization.k8s.io/v1beta1" "v1" "metrics/v1alpha1" 
                #"extensions/v1beta1" "componentconfig/v1alpha1"
kind: pod
metadata:
  name: hello-word #pod资源创建时的名称，集群内部唯一
spec：
  restartPolicy: Never  #never表明只运行该容器一次然后终止该pod
  container：
  - name: hello    #该pod内部的容器的别名
    image：                #docker镜像的名称
    command: ["/bin/echo","hello'?,'?world"] #覆盖docker容器的entrypoint。命令参数（与docker的一致）也可以用args来定义，例如：
                                             # command: ["/bin/echo"]
                                             # args: ["hello","world"]

环境变量与变量表达式：
apiVersion: v1
kind: Pod
metadata:
  name: hello-world
spec:  # specification of the pod’s contents
  restartPolicy: Never
  containers:
  - name: hello
    image: "ubuntu:14.04"
    env:
    - name: MESSAGE
      value: "hello world"
    command: ["/bin/sh","-c"]
    args: ["/bin/echo \"${MESSAGE}\""]


可以用create命令来来创建这个pod：
$ kubectl create -f ./hello-world.yaml
pods/hello-world

查看pods：
$ kubectl get pods

查看pod输出：
$ kubectl logs hello-world

删除pod：
$ kubectl delete pod hello-world
也可以用resources/name的格式删除：
$ kubectl delete pods/hello-world







etcd:
etcd是一个高可用的键值存储系统，主要用于共享配置和服务发现。etcd是由CoreOS开发并维护的，灵感来自于 ZooKeeper 和 Doozer，它使用Go语言编写，
并通过Raft一致性算法处理日志复制以保证强一致性。Raft是一个来自Stanford的新的一致性算法，适用于分布式系统的日志复制，Raft通过选举的方式来实现一致性，
在Raft中，任何一个节点都可能成为Leader。Google的容器集群管理系统Kubernetes、开源PaaS平台Cloud Foundry和CoreOS的Fleet都广泛使用了etcd。

etcd 集群的工作原理基于 raft 共识算法 (The Raft Consensus Algorithm)。etcd 在 0.5.0 版本中重新实现了 raft 算法，而非像之前那样依赖于第三方库 go-raft 。
raft 共识算法的优点在于可以在高效的解决分布式系统中各个节点日志内容一致性问题的同时，也使得集群具备一定的容错能力。
即使集群中出现部分节点故障、网络故障等问题，仍可保证其余大多数节点正确的步进。甚至当更多的节点（一般来说超过集群节点总数的一半）出现故障而导致集群不可用时，
依然可以保证节点中的数据不会出现错误的结果。





#描述pod状态kubectl describe pod my-nginx-3w5aq
#删除rc， 如果系统不停的建pod，则删除rckubectl delete rc {rc_name}
kubectl get rc --all-namespaces
kubectl scale replicationcontrollers --replicas=2 jenkins-slavekubectl rolling-update jenkins-slave --update-period=10s -f jenkins-slaves-v2.yml
#通过rest访问kuberneteshttp://10.25.23.165:8080/api/v1/namespaces/kube-system/pods/kubernetes-dashboard-v1.0.0-k0irx
kubectl delete  rc --all --namespace=kube-systemkubectl delete  pods --all --namespace=kube-systemkubectl delete  services --all --namespace=kube-system
kubectl describe rc kubernetes-dashboard --namespace=kube-system
kubectl describe pods --namespace=kube-system
kubectl logs kube-dns-v6-86kx4 kube2sky  --namespace=kube-system
kubectl -s http://k8s-master:8080 cluster-info
#在node中进入容器的方法#找到要进入的容器namesdocker ps #进入容器docker exec -it  k8s_tomcat.368537fb_javaweb-rc-nuv8e_default_6816ea53-fbe5-11e5-aa6c-005056a3e199_302ec5b9 /bin/sh
#在master中进入容器的方法kubectl exec javaweb-rc-nuv8e -c tomcat -it /bin/sh
#copy主机文件到容器目录docker cp test.txt 3796659667fa:/root#copy容器文件到主机目录docker cp 3796659667fa:/root/test.txt /paas
#列出etcd内的目录etcdctl lsetcdctl ls / --recursive

#查看本地DNS配置cat /etc/resolv.conf
#查看防火墙NAT设置iptables -t nat -L
#创建image命令docker build -t simplewar /paas/k8s/example/ #目录下要有Dockerfile文件
#查看容器详情docker inspect 容器id
#查看时间服务器ntpq -p


k8s集群：
master 10.25.31.44(centos7)
slave  10.25.31.45(centos72) 
       10.25.31.109(centos73)

1、master、slave
systemctl stop firewalld
systemctl disable firewalld
yum -y install ntp
systemctl start ntpd
systemctl enable ntpd

2、master
yum -y install etcd kubernetes
yum update -y
reboot

vi /etc/etcd/etcd.conf
ETCD_NAME=default
ETCD_DATA_DIR="/var/lib/etcd/default.etcd"
ETCD_LISTEN_CLIENT_URLS="http://0.0.0.0:2379"

vi /etc/kubernetes/apiserver
KUBE_API_ADDRESS="--address=0.0.0.0"
KUBE_API_PORT="--port=8080"
KUBELET_PORT="--kubelet_port=10250"
KUBE_ETCD_SERVERS="--etcd_servers=http://127.0.0.1:2379"
KUBE_SERVICE_ADDRESSES="--portal_net=10.254.0.0/16"
KUBE_ADMISSION_CONTROL="--admission_control=NamespaceAutoProvision,LimitRanger,ResourceQuota"
KUBE_API_ARGS=""

vi /etc/kubernetes/controller-manager
KUBELET_ADDRESSES="--machines=10.25.31.45,10.25.31.109"

for SERVICES in etcd kube-apiserver kube-controller-manager kube-scheduler; do
systemctl restart $SERVICES
systemctl enable $SERVICES
systemctl status $SERVICES
done

etcdctl mk /coreos.com/network/config '{"Network":"172.17.0.0/16"}'

3、slave
yum -y install flannel kubernetes docker
yum update -y
reboot

vi /etc/sysconfig/flanneld
FLANNEL_ETCD="http://10.25.31.44:2379"
FLANNEL_ETCD_KEY="/coreos.com/network"

vi /etc/kubernetes/config
KUBE_MASTER="--master=http://10.25.31.44:8080"

vi /etc/kubernetes/kubelet
KUBELET_ADDRESS="--address=10.25.31.109"
KUBELET_PORT="--port=10250"
KUBELET_HOSTNAME="--hostname_override=10.25.31.109"
KUBELET_API_SERVER="--api_servers=http://10.25.31.44:8080"
KUBELET_ARGS=""

for SERVICES in etcd kube-proxy kubelet docker flanneld; do
systemctl restart $SERVICES
systemctl enable $SERVICES
systemctl status $SERVICES
done

配置完毕，检测：
1、两个slave上都可以看到两个网卡：docker0和flannel0
[root@k8s_slave2 ~]# ip a | grep -E "flannel|docker"|grep inet
    inet 172.17.2.1/24 scope global docker0
    inet 172.17.2.0/16 scope global flannel0

2、在master上确认slave节点状态：
[root@k8s_master ~]#  kubectl get nodes
NAME           STATUS    AGE
10.25.31.109   Ready     11m
10.25.31.45    Ready     13h

3、检查kube-apiserver（显示ok说明服务正确）：
[root@k8s_master ~]# curl -L http://10.25.31.44:8080/healthz
ok 

创建pod：
1、创建模板文件：
vi test.yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: test-1
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: test-1
    spec:
      containers:
        - name: tomcat
          image: 10.25.31.102:5000/tomcat8

2、根据模板文件创建pod：
[root@k8s_master templates]# kubectl --server=10.25.31.44:8080 create -f test.yaml
replicationcontroller "pod-tomcat1" created

3、查询存在的pod：
[root@k8s_master templates]# kubectl --server=10.25.31.44:8080 get pods
NAME                READY     STATUS              RESTARTS   AGE
pod-tomcat1-xzkju   0/1       ContainerCreating   0          41s

4、删除pod：
[root@k8s_master ~]# kubectl delete  pod pod-tomcat1-xzkju
pod "pod-tomcat1-xzkju" deleted

5、更新pod：
kubectl replace -f test.yaml


6、获取pod对应的container的ip地址：
[root@k8s_master templates]# kubectl --server=10.25.31.44:8080 describe pod pod-tomcat1-xzkju
Name:           pod-tomcat1-xzkju
Namespace:      default
Node:           10.25.31.45/10.25.31.45
Start Time:     Tue, 24 May 2016 11:00:28 +0800
Labels:         app=lab-tomcat1
Status:         Pending
IP:             172.17.37.2
Controllers:    ReplicationController/pod-tomcat1
Containers:
......
Conditions:
......
Events:
......
上述命令返回容器的详细信息。

kubectl get nodes
kubectl get pods --namespace=default
kubectl get services
kubectl get endpoints
kubectl get services
kubectl get rc
kubectl get namespaces
kubectl label pod pod-tomcat1-o76ht role=backend  #给pod添加一个标签
kubectl label pod pod-tomcat1-o76ht role-  #在标签的key的后面添加一个减号
kubectl label pod pod-tomcat1-o76ht role=master --overwrite  #修改一个标签的值
kubectl get pods -Lrole  #查看pod列表，如果该pod有一个标签叫role，则显示该标签的值，如果没有，显示<none>
[root@k8s_master templates]# kubectl get pods -Lrole
NAME                READY     STATUS             RESTARTS   AGE       ROLE
pod-tomcat1-o76ht   1/1       Running            0          9h        backend
test-2-v1p19        0/1       ImagePullBackOff   0          10h       <none>


rest api:
curl -s -L http://10.25.23.165:8080/api/v1/services 
curl -s -L http://10.25.23.165:8080/api/v1/pods 
curl -s -L http://10.25.23.165:8080/api/v1/namespaces/default/pods/test6-rc-u9lvx
curl -s -L http://10.25.23.165:8080/api/v1/nodes
curl -s -L http://10.25.23.165:8080/api/v1/nodes/k8s-node01

创建service：
apiVersion: v1
kind: Service
metadata:
  name: tomcat-svc
  labels:
    k8s-app: tomcat-svc
    kubernetes.io/cluster-service: "true"
spec:
  clusterIP: None
  selector:
    k8s-app: lab-tomcat1
  ports:
  - port: 80
    targetPort: 8080






k8s会出现hosts错误：
# Kubernetes-managed hosts file.
127.0.0.1       localhost
::1     localhost ip6-localhost ip6-loopback
fe00::0 ip6-localnet
fe00::0 ip6-mcastprefix
fe00::1 ip6-allnodes
fe00::2 ip6-allrouters
 devops-rc-6wvr2

@ALL，我们在DevOps Portal里面可以增加一个hosts配置项, 老尹拿到后可以先找到部署container，
再执行docker exec -it 09a25d965c29 /bin/sh -c "echo 127.0.0.1 test >>  /etc/hosts" 这个命令将hosts加进去





